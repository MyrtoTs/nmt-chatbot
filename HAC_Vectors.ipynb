{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.cluster import hierarchy as hac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf idf Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(count_matrix):\n",
    "    ''' Takes as input a count matrix \n",
    "        of ter occurencies in documents\n",
    "        'doc-term' (N_doc x N_term)\n",
    "        and returns a tfidf matrix of \n",
    "        same dimensions'''\n",
    "        \n",
    "    [total_number_of_documents, total_number_of_terms] = count_matrix.shape\n",
    "    \n",
    "    words_per_subreddit = np.sum(count_matrix, axis = 0)\n",
    "    tf = count_matrix/words_per_subreddit\n",
    "    \n",
    "    subreddits_per_word = np.count_nonzero(count_matrix, axis = 1)\n",
    "    df = subreddits_per_word/total_number_of_documents\n",
    "    idf = np.log(np.reciprocal(df))   \n",
    "    \n",
    "    tf_idf = tf*idf[:,np.newaxis]\n",
    "    return(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries and Count Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments:\n",
      "5621488\n",
      "\n",
      "Vocabulary size:\n",
      "15003\n",
      "\n",
      "Labels dictionary size:\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "with open('data.train', 'r', 10000) as t:\n",
    "    lines = t.read().splitlines()\n",
    "    \n",
    "print('Comments:')\n",
    "print(len(lines))\n",
    "\n",
    "voc_dict = {}\n",
    "voc_dict_inv = {}\n",
    "i = 0\n",
    "with open('vocab.bpe.from','r') as f:\n",
    "    keys = f.read().splitlines()\n",
    "    for key in keys:\n",
    "        if i>2 :\n",
    "            voc_dict[i-3] = key\n",
    "            voc_dict_inv[key] = i-3            \n",
    "#             print(key)\n",
    "        i+=1\n",
    "print('\\nVocabulary size:')\n",
    "print(i)\n",
    "\n",
    "with open('subreddits.train','r') as l:\n",
    "    ls = l.read().splitlines()\n",
    "    label_names = list(set(ls))\n",
    "        \n",
    "# print(label_names)\n",
    "\n",
    "labels_dict = {}\n",
    "labels_dict_inv = {}\n",
    "for i, label_name in enumerate(label_names):\n",
    "    labels_dict[i] = label_name\n",
    "    labels_dict_inv[label_name] = i\n",
    "\n",
    "print('\\nLabels dictionary size:')\n",
    "print(len(labels_dict))\n",
    "# print(labels_dict_inv)\n",
    "\n",
    "subredditsXvocwords = np.zeros([len(labels_dict), 15000])\n",
    "\n",
    "for i, (comment, label) in enumerate(zip(lines,ls)):\n",
    "    words = comment.split(' ')\n",
    "    for w in words:\n",
    "        subredditsXvocwords[labels_dict_inv[label], voc_dict_inv[w]] += 1\n",
    "\n",
    "del(lines)\n",
    "del(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_tf_idf = tfidf(subredditsXvocwords)\n",
    "z = hac.linkage(subreddit_tf_idf, method = 'complete', metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes list of N clusters after (638-N) unions\n",
    "\n",
    "k = np.zeros([len(labels_dict)-1, 2])\n",
    "k[:,0]=z[:,0]\n",
    "k[:,1]=z[:,1]\n",
    "k = k.astype(int)\n",
    "l = k.tolist()\n",
    "m = [[] for x in range(2*len(labels_dict)-1)]\n",
    "for i in range(len(labels_dict)):\n",
    "    m[i].append(i)\n",
    "\n",
    "N=40\n",
    "\n",
    "for k in range(len(labels_dict)-N):\n",
    "    for j in range(2):\n",
    "        if l[k][j]<len(labels_dict):\n",
    "            m[k+len(labels_dict)].append(l[k][j])\n",
    "            m[l[k][j]]=[]\n",
    "        else:\n",
    "            for i in m[l[k][j]]:\n",
    "                m[k+len(labels_dict)].append(i)\n",
    "            m[l[k][j]]=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment below to print dendrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prints N dendrograms (one per Cluster)\n",
    "\n",
    "# subreddit_groupsXvocwords = np.zeros([N, 15000])\n",
    "\n",
    "# counter=1\n",
    "# subreddits_groups = []\n",
    "# for t in m:\n",
    "#     if t!=[]:\n",
    "#         print('\\n Cluster '+str(counter) + ' with '+ str(len(t)) +' subreddits')\n",
    "#         for s in t:\n",
    "#             print(labels_dict[s], end =', ')\n",
    "#         if len(t) > 1 :\n",
    "#             matrix = np.zeros([len(t),15000])\n",
    "#             matrix_labels = []\n",
    "#             for n, s in enumerate(t):\n",
    "#                 matrix_labels.append(labels_dict[s])\n",
    "#                 matrix[n,:] += subreddit_tf_idf[s,:]\n",
    "#             sub_z = hac.linkage(matrix, method = 'complete', metric = 'cosine')\n",
    "            \n",
    "# #             mpl.rcParams['axes.titlesize'] = 60\n",
    "\n",
    "#             if len(t)>150:\n",
    "#                 plt.figure(figsize=(10, 60))\n",
    "#                 hac.dendrogram(sub_z, labels = matrix_labels, orientation = 'left')\n",
    "#                 ax = plt.gca()\n",
    "#                 ax.tick_params(axis='x', which='major', labelsize=15)\n",
    "#                 ax.tick_params(axis='y', which='major', labelsize=18)\n",
    "#                 plt.title('Hierarchical Clustering Dendrogram of Cluster '+ str(counter), fontsize = 25)\n",
    "#     #             plt.xlabel('Subreddits')\n",
    "#                 plt.ylabel('Distance between Subreddits')\n",
    "                \n",
    "#             else:\n",
    "#                 plt.figure(figsize=(25, 8))\n",
    "#                 if len(t)>=10 and len(t)<30:   \n",
    "#                     hac.dendrogram(sub_z, labels = matrix_labels, leaf_rotation=45.)\n",
    "#                 else:\n",
    "#                     hac.dendrogram(sub_z, labels = matrix_labels)\n",
    "#                 ax = plt.gca()\n",
    "#                 ax.tick_params(axis='x', which='major', labelsize=18)\n",
    "#                 ax.tick_params(axis='y', which='major', labelsize=15)\n",
    "#                 plt.title('Hierarchical Clustering Dendrogram of Cluster '+ str(counter), fontsize = 25)\n",
    "#     #             plt.xlabel('Subreddits')\n",
    "#                 plt.ylabel('Distance between Subreddits')\n",
    "\n",
    "#             plt.show()\n",
    "                \n",
    "#             subreddits_groups.append('Cluster '+str(counter))\n",
    "#         else:\n",
    "#             subreddits_groups.append(t)\n",
    "            \n",
    "#         counter+=1               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAC vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_groupsXvocwords = np.zeros([N, 15000])\n",
    "\n",
    "counter=0\n",
    "subreddits_groups = []\n",
    "for t in m:\n",
    "    if t!=[]:\n",
    "        for s in t:\n",
    "            subreddit_groupsXvocwords[counter,:] += subredditsXvocwords[s,:]\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAC_matrix = tfidf(np.transpose(subreddit_groupsXvocwords))\n",
    "# HAC_matrix_ordered\n",
    "HAC_matrix_ordered = np.zeros([15000,N])\n",
    "\n",
    "HAC_matrix_ordered[0,:]=np.full((40),0.025)\n",
    "HAC_matrix_ordered[1,:]=np.full((40),0.025)\n",
    "HAC_matrix_ordered[2,:]=np.full((40),0.025)\n",
    "\n",
    "with open('vocab.bpe.from','r') as f:\n",
    "    keys = f.read().splitlines()\n",
    "    for i, key in enumerate(keys):\n",
    "        if i>2:\n",
    "            HAC_matrix_ordered[i-3,:] += HAC_matrix[voc_dict_inv[key],:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HAC_vectors.pickle', 'wb') as h:\n",
    "    pickle.dump(HAC_matrix_ordered, h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
