{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928\n",
      "584828\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import random\n",
    "import string\n",
    "\n",
    "database_path = '/home/myrto/pink_dir/github/DataFiles/decem_17_db.db'\n",
    "\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"SELECT subreddit FROM parents WHERE score>=130 GROUP BY subreddit HAVING SUM(length)>=1000\"\"\")\n",
    "ps = cursor.fetchall()\n",
    "print(len(ps))\n",
    "\n",
    "cursor.execute(\"\"\"SELECT * FROM parents WHERE score>=130\"\"\")\n",
    "ps2 = cursor.fetchall()\n",
    "print(len(ps2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdviceAnimals\n",
      "AskMen\n",
      "AskReddit\n",
      "BlackPeopleTwitter\n",
      "CFB\n",
      "DestinyTheGame\n",
      "Games\n",
      "IAmA\n",
      "JUSTNOMIL\n",
      "MMA\n",
      "Overwatch\n",
      "Showerthoughts\n",
      "SquaredCircle\n",
      "StarWars\n",
      "The_Donald\n",
      "WTF\n",
      "WritingPrompts\n",
      "aww\n",
      "funny\n",
      "gaming\n",
      "gifs\n",
      "hearthstone\n",
      "hiphopheads\n",
      "leagueoflegends\n",
      "legaladvice\n",
      "mildlyinteresting\n",
      "movies\n",
      "nba\n",
      "news\n",
      "nfl\n",
      "nottheonion\n",
      "pics\n",
      "politics\n",
      "relationships\n",
      "soccer\n",
      "technology\n",
      "television\n",
      "todayilearned\n",
      "videos\n",
      "worldnews\n",
      "['AdviceAnimals', 'AskMen', 'AskReddit', 'BlackPeopleTwitter', 'CFB', 'DestinyTheGame', 'Games', 'IAmA', 'JUSTNOMIL', 'MMA', 'Overwatch', 'Showerthoughts', 'SquaredCircle', 'StarWars', 'The_Donald', 'WTF', 'WritingPrompts', 'aww', 'funny', 'gaming', 'gifs', 'hearthstone', 'hiphopheads', 'leagueoflegends', 'legaladvice', 'mildlyinteresting', 'movies', 'nba', 'news', 'nfl', 'nottheonion', 'pics', 'politics', 'relationships', 'soccer', 'technology', 'television', 'todayilearned', 'videos', 'worldnews']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "labels_list=[]\n",
    "\n",
    "for k in ps:\n",
    "    count_words = 0\n",
    "    num_doc = 0\n",
    "    for parent_id, reply_id, parent_body, parent_subreddit, parent_utc_time, parent_score, parent_length in ps2:\n",
    "        if parent_subreddit == (str(k)[2:-3]):\n",
    "            cursor.execute(\"\"\"SELECT * FROM replies WHERE id='{}'\"\"\".format(reply_id))\n",
    "            reply = cursor.fetchone()\n",
    "            if reply:\n",
    "                reply_id, reply_body, reply_subreddit, reply_utc_time, reply_score, reply_length = reply\n",
    "                to_add = parent_body + reply_body\n",
    "                if not ('http' in to_add):\n",
    "                    count_words = count_words + parent_length + reply_length\n",
    "        if count_words >= 2000:\n",
    "            count_words = 0\n",
    "            num_doc += 1\n",
    "    if num_doc > 50:\n",
    "        new_label = str(k)[2:-3] \n",
    "        labels_list.append(new_label)\n",
    "        print(new_label)\n",
    "print(labels_list)\n",
    "print(len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data.train', 'w', buffering=10000)\n",
    "labels = open('labels.train', 'w', buffering=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parent_id, reply_id, parent_body, parent_subreddit, parent_utc_time, parent_score, parent_length in ps2:\n",
    "    cursor.execute(\"\"\"SELECT * FROM replies WHERE id='{}'\"\"\".format(reply_id))\n",
    "    reply = cursor.fetchone()\n",
    "    if reply:\n",
    "        reply_id, reply_body, reply_subreddit, reply_utc_time, reply_score, reply_length = reply     \n",
    "        if parent_subreddit in labels_list:                 \n",
    "            file.write(parent_body + '\\n')\n",
    "            file.write(reply_body + '\\n') \n",
    "            labels.write(parent_subreddit +'\\n')\n",
    "            labels.write(reply_subreddit + '\\n')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()\n",
    "labels.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Κρατήσαμε μόνο τα σχόλια από τα 40 συνηθέστερα subreddits. Ας κάνουμε και cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R', 'N', 'Z', '”', '%', 'g', 'S', ';', '“', '_', '4', '$', 'F', '0', 'I', 'w', '@', 'M', ':', '\\x0c', 'e', '5', 'P', 'y', 'K', 'W', '8', 'E', ',', '>', '\\t', 'r', '\"', '2', 'j', '\\r', 'l', 'L', '\\x0b', '7', 'z', 'C', '&', '(', '{', 'G', '3', '*', 't', 'q', 'X', 'H', 'o', '\\n', '`', '-', 'm', 'T', '!', 'n', '\\\\', 'x', 'U', '^', '+', '/', 'Y', '=', 'c', '<', 'D', 'V', ' ', 'B', 'a', 'd', '~', '}', 'k', 'f', '‘', \"'\", '’', '[', '1', 'u', 'J', ']', '?', 'Q', 'h', '#', 'b', 'i', 'p', 'O', '9', 'v', '|', '6', 'A', 's', '.', ')'}\n"
     ]
    }
   ],
   "source": [
    "a = string.printable + '’‘“”'\n",
    "eng_char = set(a)\n",
    "print(eng_char)\n",
    "\n",
    "file = open('data.train')\n",
    "clean_file = open('clean_data.train', 'w')\n",
    "\n",
    "labels = open('labels.train')\n",
    "clean_labels = open('clean_labels.train', 'w')\n",
    "\n",
    "count = 0\n",
    "count_clear = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment pairs before cleaning631120\n",
      "Clean Comment pairs608979\n"
     ]
    }
   ],
   "source": [
    "for f, l in zip(file, labels):\n",
    "    count += 1\n",
    "    if ('http' not in f) and (f != '\\n'):\n",
    "        comment = set(f)\n",
    "        if comment.issubset(eng_char):\n",
    "            count_clear +=1            \n",
    "            while '....' in (f):\n",
    "                f=f.replace('....','...')            \n",
    "            clean_file.write(f)\n",
    "            clean_labels.write(l)\n",
    "\n",
    "print(\"Comment pairs before cleaning{}\".format(count)) \n",
    "print(\"Clean Comment pairs{}\".format(count_clear)) \n",
    "\n",
    "file.close()\n",
    "labels.close()\n",
    "clean_file.close()\n",
    "clean_labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608979\n"
     ]
    }
   ],
   "source": [
    "file2 = open('data.to','w')\n",
    "for i in range(count_clear):\n",
    "    file2.write('.\\n')\n",
    "print(count_clear)\n",
    "\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Μεταφέρω το clean_data.train και το data.to  στο /home/myrto/pink_dir/github/myrto-tsokanaridou-chatbot/new_data και το μετονομάζω σε train.from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/myrto/pink_dir/github/myrto-tsokanaridou-chatbot/setup\n"
     ]
    }
   ],
   "source": [
    "cd /home/myrto/pink_dir/github/myrto-tsokanaridou-chatbot/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "Preparing training set from raw set\u001b[39m\n",
      "File: \u001b[32mtst.from\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00, 10.33 lines/s]\n",
      "File: \u001b[32mdev.from\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00,  9.13 lines/s]\n",
      "File: \u001b[32mtrain.from\u001b[39m\n",
      " 44%|############4               | 270000/608979 [00:09<00:12, 28159.53 lines/s]/home/myrto/p3venv/lib/python3.5/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|############################| 608979/608979 [00:20<00:00, 40948.56 lines/s]\n",
      "File: \u001b[32mtst.to\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00,  5.67 lines/s]\n",
      "File: \u001b[32mdev.to\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00, 11.00 lines/s]\n",
      "File: \u001b[32mtrain.to\u001b[39m\n",
      "100%|###########################| 608979/608979 [00:01<00:00, 327033.51 lines/s]\n",
      "\u001b[32m\n",
      "Learning BPE\u001b[39m\n",
      "Building temporary vocab (from)\n",
      "195086 tokens [00:01, 110355.45 tokens/s]\n",
      "Learning BPE for vocab of 15000 tokens\n",
      "100%|###############################| 15000/15000 [00:22<00:00, 654.60 tokens/s]\n",
      "\u001b[32m\n",
      "Applying BPE\u001b[39m\n",
      "File: \u001b[32mtrain.bpe.from\u001b[39m\n",
      "100%|############################| 608979/608979 [00:15<00:00, 39087.53 lines/s]\n",
      "File: \u001b[32mdev.bpe.from\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00, 24.62 lines/s]\n",
      "File: \u001b[32mtst.bpe.from\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00, 23.70 lines/s]\n",
      "File: \u001b[32mtrain.bpe.to\u001b[39m\n",
      "100%|###########################| 608979/608979 [00:01<00:00, 502062.39 lines/s]\n",
      "File: \u001b[32mdev.bpe.to\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00, 26.79 lines/s]\n",
      "File: \u001b[32mtst.bpe.to\u001b[39m\n",
      "100%|#########################################| 1/1 [00:00<00:00, 23.97 lines/s]\n",
      "\u001b[32m\n",
      "Postprocessing and saving vocabs\u001b[39m\n",
      "File: \u001b[32mvocab.bpe.from\u001b[39m\n",
      "\u001b[32m\n",
      "Writing pbtxt file\u001b[39m\n",
      "\u001b[32m\n",
      "All done\u001b[39m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
