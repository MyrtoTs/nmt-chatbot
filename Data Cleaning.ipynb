{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "database_path = '/home/myrto/pink_dir/github/DataFiles/december_17_db.db'\n",
    "\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits are 17046 with 3701464 comment pairs.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"SELECT * FROM comment_pairs\"\"\")\n",
    "ps = cursor.fetchall()\n",
    "sum_of_comment_pairs = len(ps)\n",
    "\n",
    "cursor.execute(\"\"\"SELECT subreddit, COUNT(subreddit) FROM comment_pairs GROUP BY subreddit ORDER BY COUNT(subreddit) DESC\"\"\")\n",
    "ps = cursor.fetchall()\n",
    "print('Subreddits are '+ str(len(ps)) +' with '+ str(sum_of_comment_pairs)+ ' comment pairs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα συνολικά subreddits είναι 17.046 με 3.701.464 ζεύγη σχολίων. Είναι λογικό να σκεφτούμε, πως για να βγάλουμε κάποιο συμπέρασμα για τη θεματολογία του subreddit πρέπει να έχουμε αρκετά σχόλια που να ανήκουν σε αυτό."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits with less than 10 comment pairs: 9690\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"SELECT subreddit, COUNT(subreddit) FROM comment_pairs GROUP BY subreddit HAVING COUNT(subreddit)<10\"\"\")\n",
    "ps = cursor.fetchall()\n",
    "print('Subreddits with less than 10 comment pairs: ' + str(len(ps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρούμε, ωστόσο, ότι παραπάνω από τα μισά subreddits δεν έχουν ούτε 10 ζεύγη σχολίων, πράγμα που μας οδηγεί στο να βάλουμε ένα κατώφλι στα subreddits ώστε να απορρίπτουμε εκείνα με μικρότερο πλήθος σχολίων από αυτό. Επιλέξαμε τα 1.000 ζεύγη σχολίων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 638 subreddits to be used with total number of comments: 2810744\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"SELECT subreddit, COUNT(subreddit) FROM comment_pairs GROUP BY subreddit HAVING COUNT(subreddit)>1000 ORDER BY COUNT(subreddit) DESC\"\"\")\n",
    "big_enough_subreddits = cursor.fetchall()\n",
    "\n",
    "subreddits, count_comments = map(list, zip(*big_enough_subreddits))\n",
    "print('There are ' + str(len(big_enough_subreddits)) +' subreddits to be used with total number of comments: ' +str(sum(count_comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# sb = pd.DataFrame(subreddits[0:15], columns = ['Subreddit'])\n",
    "# cc = pd.DataFrame(count_comments[0:15], columns = ['Comment pairs'])\n",
    "# pd.concat([sb,cc], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"SELECT p.parent_body, p.reply_body, p.subreddit FROM comment_pairs p JOIN\n",
    "                ( SELECT subreddit FROM comment_pairs GROUP BY subreddit HAVING COUNT(subreddit) > 1000 ) b \n",
    "                ON p.subreddit = b.subreddit\"\"\")\n",
    "\n",
    "ps = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data.train', 'w', buffering=10000)\n",
    "labels = open('labels.train', 'w', buffering=10000)\n",
    "\n",
    "for parent_body, reply_body, subreddit in ps:\n",
    "    file.write(parent_body + reply_body + '\\n')\n",
    "    labels.write(subreddit + '\\n')       \n",
    "\n",
    "file.close()\n",
    "labels.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην συνέχεια, το data.train μπαίνει ως αρχείο εκπαίδευσης στο new_data του nmt-chatbot, γίνεται tokenizarion και το παίρνουμε ωε train.bpe.from από τον φάκελο data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
